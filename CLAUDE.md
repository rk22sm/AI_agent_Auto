# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is an **Autonomous Claude Agent Plugin** that demonstrates true autonomous AI behavior through pattern learning, skill auto-selection, background task execution, and comprehensive quality control. The plugin implements a "Brain-Hand Collaboration" model where the orchestrator agent makes strategic decisions autonomously while specialized agents and skills execute tasks with focused expertise.

**Platform**: Claude Code CLI only (uses subagents, not compatible with claude.ai web/mobile)

## Development Guidelines

### ğŸ—ï¸ Development & Distribution Architecture

**IMPORTANT**: This project uses a dual-mode dashboard system that supports both development and distribution environments. Before making changes to dashboard functionality, please read:

- **[Development & Distribution Architecture](docs/DEVELOPMENT_DISTRIBUTION_ARCHITECTURE.md)** - Complete guide to dual-mode system
- **[Distribution Validation Report](DISTRIBUTION_VALIDATION_REPORT.md)** - Validation results and testing procedures

#### Quick Reference for Dashboard Changes

**Development Mode**: `lib/dashboard.py`
- Primary development environment
- Direct plugin access
- Unified storage integration

**Distribution Mode**: `.claude-patterns/dashboard.py`
- User distribution (auto-copied)
- Local copy optimization
- Graceful fallback handling

**Testing Both Modes**:
```bash
# Test development mode
python lib/dashboard.py --no-browser --port 5000

# Test distribution mode
cp lib/dashboard.py .claude-patterns/dashboard.py
python .claude-patterns/dashboard.py --no-browser --port 5001
```

## Architecture

### Enhanced Two-Tier Agent Architecture (v6.1.1+)

**ENHANCED**: Optimized separation of analysis and execution with advanced multi-project learning and performance optimization systems.

#### **Tier 1: Analysis & Recommendation Agents** (The "Brain")
These agents analyze, suggest, and provide insights **WITHOUT executing changes**:

- **code-analyzer**: Analyzes code structure and identifies issues
- **smart-recommender**: Suggests optimal workflows based on patterns
- **security-auditor**: Identifies security vulnerabilities
- **performance-analytics**: Analyzes performance trends
- **pr-reviewer**: Reviews pull requests and suggests improvements
- **learning-engine**: Captures patterns and learns from outcomes
- **validation-controller**: Validates approaches before execution

#### **Tier 2: Execution & Decision Agents** (The "Hand")
These agents **evaluate Tier 1 recommendations, make decisions, and execute changes**:

- **quality-controller**: Evaluates quality and executes auto-fixes
- **test-engineer**: Creates and fixes tests based on analysis
- **frontend-analyzer**: Fixes TypeScript/React issues
- **documentation-generator**: Creates documentation
- **build-validator**: Validates and fixes build configurations
- **git-repository-manager**: Executes git operations
- **api-contract-validator**: Synchronizes API contracts
- **gui-validator**: Validates and fixes GUI issues
- **dev-orchestrator**: Orchestrates development workflows
- **version-release-manager**: Manages releases
- **workspace-organizer**: Organizes workspace files
- **claude-plugin-validator**: Validates plugin compliance

#### **Orchestrator**: Master Controller
`agents/orchestrator.md` - Coordinates the two-tier workflow:
1. Delegates to Tier 1 for analysis and recommendations
2. Loads user preferences
3. Delegates to Tier 2 for execution with context
4. Captures feedback loops between tiers
5. Records performance metrics and user interactions

#### **Automatic Learning Systems**

**Pattern Learning**: `.claude-patterns/patterns.json` - Stores successful approaches
**Agent Feedback**: `.claude-patterns/agent_feedback.json` - Cross-tier communication
**Agent Performance**: `.claude-patterns/agent_performance.json` - Individual agent metrics
**User Preferences**: `.claude-patterns/user_preferences.json` - Learned user preferences

### Component Structure

```
.claude-plugin/plugin.json          # Plugin manifest with metadata (v6.1.1)

agents/                              # 22 specialized subagents
â”œâ”€â”€ orchestrator.md                 # Main autonomous controller
â”œâ”€â”€ code-analyzer.md                # Code structure analysis
â”œâ”€â”€ quality-controller.md           # Quality assurance with auto-fix
â”œâ”€â”€ background-task-manager.md      # Parallel background tasks
â”œâ”€â”€ test-engineer.md                # Test generation, fixing, DB isolation (enhanced v2.0)
â”œâ”€â”€ documentation-generator.md      # Documentation maintenance
â”œâ”€â”€ learning-engine.md              # Automatic learning (v1.1+)
â”œâ”€â”€ performance-analytics.md        # Performance insights (v1.2+)
â”œâ”€â”€ smart-recommender.md            # Intelligent recommendations (v1.3+)
â”œâ”€â”€ validation-controller.md        # Proactive validation & error prevention (v1.7+)
â”œâ”€â”€ frontend-analyzer.md            # TypeScript, React, build validation (v2.0)
â”œâ”€â”€ api-contract-validator.md       # API synchronization & type generation (v2.0)
â”œâ”€â”€ build-validator.md              # Build configuration validation (v2.0)
â”œâ”€â”€ version-release-manager.md      # Version and release management (v4.1.0)
â”œâ”€â”€ report-management-organizer.md  # Report organization and management (v5.x)
â”œâ”€â”€ gui-validator.md                # GUI debugging and validation (v5.x)
â”œâ”€â”€ dev-orchestrator.md             # Development workflow orchestration (v5.x)
â”œâ”€â”€ claude-plugin-validator.md      # Plugin validation specialist (v2.1.2)
â”œâ”€â”€ [Plus 7 additional specialized agents for workflow automation]

skills/                              # 17 knowledge packages
â”œâ”€â”€ pattern-learning/               # Pattern recognition system
â”œâ”€â”€ code-analysis/                  # Code analysis methodologies
â”œâ”€â”€ quality-standards/              # Quality benchmarks
â”œâ”€â”€ testing-strategies/             # Test design patterns
â”œâ”€â”€ documentation-best-practices/   # Documentation standards
â”œâ”€â”€ validation-standards/           # Tool validation & consistency checks (v1.7+)
â”œâ”€â”€ fullstack-validation/           # Full-stack validation methodology (v2.0)
â”œâ”€â”€ model-detection/                # Cross-model compatibility detection (v3.0+)
â”œâ”€â”€ performance-scaling/            # Model-specific performance optimization (v3.0+)
â”œâ”€â”€ contextual-pattern-learning/    # Enhanced pattern learning with context (v3.0+)
â”œâ”€â”€ ast-analyzer/                   # Abstract syntax tree analysis (v3.0+)
â”œâ”€â”€ security-patterns/              # Security analysis and patterns (v3.0+)
â”œâ”€â”€ [Plus 5 additional specialized skills]

commands/                            # 39 slash commands (8 categories with colon notation)
â”œâ”€â”€ dev/                           # Development commands
â”‚   â”œâ”€â”€ auto.md                    # Autonomous development workflow (/dev:auto)
â”‚   â”œâ”€â”€ commit.md                  # Intelligent commit management (/dev:commit) (NEW v5.4+)
â”‚   â”œâ”€â”€ release.md                 # Release management workflow (/dev:release)
â”‚   â”œâ”€â”€ model-switch.md            # Model switching (/dev:model-switch)
â”‚   â””â”€â”€ pr-review.md              # Pull request review (/dev:pr-review)
â”œâ”€â”€ analyze/                       # Analysis commands
â”‚   â”œâ”€â”€ project.md                 # Autonomous project analysis (/analyze:project)
â”‚   â”œâ”€â”€ quality.md                 # Comprehensive quality control (/analyze:quality)
â”‚   â”œâ”€â”€ static.md                  # Static code analysis (/analyze:static)
â”‚   â”œâ”€â”€ dependencies.md           # Dependency vulnerability scanning (/analyze:dependencies)
â”‚   â”œâ”€â”€ explain.md                 # Explain task without modification (NEW v5.4+)
â”‚   â””â”€â”€ repository.md             # Analyze external repositories (NEW v5.4+)
â”œâ”€â”€ validate/                       # Validation commands
â”‚   â”œâ”€â”€ all.md                     # Comprehensive validation audit (/validate:all)
â”‚   â”œâ”€â”€ fullstack.md               # Full-stack validation & auto-fix (/validate:fullstack)
â”‚   â”œâ”€â”€ plugin.md                 # Claude plugin validation (/validate:plugin)
â”‚   â””â”€â”€ patterns.md               # Pattern validation (/validate:patterns)
â”œâ”€â”€ debug/                          # Debugging commands
â”‚   â”œâ”€â”€ eval.md                    # Evaluation debugging (/debug:eval)
â”‚   â””â”€â”€ gui.md                    # GUI debugging (/debug:gui)
â”œâ”€â”€ learn/                          # Learning commands
â”‚   â”œâ”€â”€ init.md                    # Initialize pattern learning (/learn:init)
â”‚   â”œâ”€â”€ analytics.md               # Learning analytics (/learn:analytics)
â”‚   â”œâ”€â”€ performance.md             # Performance analytics dashboard (/learn:performance)
â”‚   â”œâ”€â”€ predict.md                # Predictive analytics (/learn:predict)
â”‚   â”œâ”€â”€ history.md                 # Learn from repository history (NEW v5.4+)
â”‚   â””â”€â”€ clone.md                  # Clone features from external repos (NEW v5.4+)
â”œâ”€â”€ workspace/                      # Workspace commands
â”‚   â”œâ”€â”€ organize.md                # Workspace organization (/workspace:organize)
â”‚   â”œâ”€â”€ reports.md                 # Report organization (/workspace:reports)
â”‚   â”œâ”€â”€ improve.md                # Plugin improvement (/workspace:improve)
â”‚   â”œâ”€â”€ update-readme.md          # Intelligent README updates (NEW v5.4+)
â”‚   â””â”€â”€ update-about.md           # GitHub About section updates (NEW v5.4+)
â”œâ”€â”€ monitor/                        # Monitoring commands
â”‚   â”œâ”€â”€ dashboard.md               # Real-time monitoring dashboard (/monitor:dashboard)
â”‚   â””â”€â”€ recommend.md               # Smart workflow recommendations (/monitor:recommend)
â””â”€â”€ git-release-workflow.md        # Git release workflow (special case)

patterns/                            # Auto-fix pattern database (NEW v2.0)
â””â”€â”€ autofix-patterns.json           # 24 patterns with 89% avg success rate

lib/                                 # Python utilities and path resolution (v5.6+)
â”œâ”€â”€ exec_plugin_script.py           # Cross-platform script executor
â”œâ”€â”€ plugin_path_resolver.py         # Dynamic plugin path discovery
â”œâ”€â”€ agent_feedback_system.py        # Agent-to-agent feedback (NEW v5.9+)
â”œâ”€â”€ agent_performance_tracker.py    # Individual agent metrics (NEW v5.9+)
â”œâ”€â”€ user_preference_learner.py      # User preference learning (NEW v5.9+)
â”œâ”€â”€ dashboard_unified_adapter.py    # Dashboard integration with two-tier metrics
â””â”€â”€ [110+ utility scripts]
```

### Cross-Platform Plugin Path Resolution (v5.6+)

The plugin uses a three-layer architecture to execute Python scripts across all platforms and installation methods:

**Layer 1: Script Executor** (`lib/exec_plugin_script.py`):
- Automatically finds plugin installation location
- Executes target scripts from `lib/` directory
- Forwards all arguments to target script
- Works in development, marketplace, and system-wide installations

**Layer 2: Path Resolver** (`lib/plugin_path_resolver.py`):
- Dynamically discovers plugin installation across platforms
- Checks development directories (current working directory and parents)
- Checks marketplace paths (`~/.claude/plugins/marketplaces/LLM-Autonomous-Agent-Plugin-for-Claude/`)
- Checks local installations (`~/.claude/plugins/autonomous-agent/`)
- Checks system-wide paths (`/usr/local/share/claude/plugins/`, Windows Program Files, etc.)
- Respects `CLAUDE_PLUGIN_PATH` environment variable for custom locations
- Validates installation by checking for `.claude-plugin/plugin.json`

**Layer 3: Command Execution**:
```bash
# All slash commands use this pattern:
python lib/exec_plugin_script.py {script_name} {arguments}

# Example:
python lib/exec_plugin_script.py dashboard.py --port 5000
```

**Why This Matters**:
- âœ… No hardcoded paths - works on any user's machine
- âœ… Cross-platform - Windows, Linux, macOS
- âœ… Installation-agnostic - development, marketplace, system-wide
- âœ… User-independent - no assumptions about home directories
- âœ… Clear errors - shows searched locations if plugin not found

**Key Files**:
- `lib/exec_plugin_script.py` - Execute scripts with automatic path resolution
- `lib/plugin_path_resolver.py` - Find plugin installation dynamically
- `docs/CROSS_PLATFORM_PLUGIN_ARCHITECTURE.md` - Complete architecture documentation
- `docs/COMMAND_UPDATE_GUIDE.md` - Guide for updating slash commands

## Key Architectural Principles

### 1. True Autonomous Operation

The orchestrator agent operates **without human approval** at each step:
- Analyzes tasks independently
- Auto-selects relevant skills based on context and history
- Delegates to specialized agents autonomously
- Runs quality checks automatically
- Self-corrects when quality score < 70/100
- Stores learned patterns after every task

### 2. Pattern Learning (Project Level)

**Location**: `.claude-patterns/` (auto-created in user projects, NOT in this repo)

**Pattern Database Schema**:
```json
{
  "project_context": {
    "detected_languages": ["python", "javascript"],
    "frameworks": ["flask", "react"],
    "project_type": "web-application"
  },
  "patterns": [{
    "task_type": "refactoring",
    "context": {...},
    "execution": {
      "skills_used": ["code-analysis", "quality-standards"],
      "agents_delegated": ["code-analyzer"]
    },
    "outcome": {
      "success": true,
      "quality_score": 96
    },
    "reuse_count": 5
  }],
  "skill_effectiveness": {
    "code-analysis": {
      "success_rate": 0.93,
      "recommended_for": ["refactoring", "bug-fix"]
    }
  }
}
```

### 3. Skill Auto-Selection Algorithm

**Decision Process**:
1. Analyze task â†’ Extract type, context, complexity
2. Query `.claude-patterns/patterns.json` for similar past tasks
3. Rank patterns by `success_rate * reuse_count`
4. Extract skills from top 3 patterns
5. Weight by skill effectiveness scores
6. Auto-load ordered skill list

**Example**: "Refactor auth module" â†’ Auto-loads: code-analysis, quality-standards, pattern-learning

### 4. Quality Score System

```
Quality Score (0-100) =
  Tests Passing      (30 points) +
  Standards          (25 points) +
  Documentation      (20 points) +
  Pattern Adherence  (15 points) +
  Code Metrics       (10 points)

Threshold: 70/100
If score < 70: Auto-correction loop triggered
If score â‰¥ 70: Task marked complete
```

### 5. Agent Delegation Strategy

**Orchestrator delegates based on task type**:
- Code analysis â†’ `code-analyzer` agent
- Quality control â†’ `quality-controller` agent (with auto-fix loop)
- Testing â†’ `test-engineer` agent
- Documentation â†’ `documentation-generator` agent
- Long-running tasks â†’ `background-task-manager` agent (parallel execution)

## Development Commands

### Testing the Plugin

Since this is a plugin definition (not executable code), testing involves:

1. **Install plugin in Claude Code**:
   ```bash
   # Copy to Claude Code plugins directory
   cp -r . ~/.config/claude/plugins/autonomous-agent/
   ```

2. **Test in a sample project**:
   ```bash
   # Navigate to test project
   cd /path/to/test-project

   # Initialize pattern learning
   /learn:init

   # Run autonomous analysis
   /analyze:project

   # Test quality control
   /analyze:quality
   ```

3. **Verify pattern storage**:
   ```bash
   # Check pattern database was created
   cat .claude-patterns/patterns.json
   ```

### Modifying Plugin Components

**IMPORTANT**: Claude Code uses convention-based discovery. The `plugin.json` manifest only contains metadata and does NOT explicitly list components.

**Adding a new agent**:
1. Create `agents/new-agent.md` following the YAML frontmatter format
2. Claude Code automatically discovers it from the `agents/` directory
3. Reference relevant skills in the agent's system prompt

**Adding a new skill**:
1. Create `skills/new-skill/SKILL.md` with YAML frontmatter
2. Claude Code automatically discovers it from `skills/*/SKILL.md`
3. Reference from agent system prompts using skill name

**Adding a new command**:
1. Create `commands/new-command.md` with command description
2. Claude Code automatically discovers it from the `commands/` directory

## Important Patterns and Conventions

### Agent System Prompt Structure

All agents follow this pattern:
```markdown
---
name: agent-name
description: When to invoke this agent (action-oriented)
tools: Read,Write,Edit,Bash,Grep,Glob  # Optional
model: inherit  # Optional
---

# Agent Title

Core responsibilities...

## Skills Integration
Reference skills by name...

## Approach
Detailed instructions...

## Handoff Protocol
How to return results...
```

### Skill Structure

All skills follow this pattern:
```markdown
---
name: Skill Name
description: What this skill provides (200 char max)
version: 1.0.0
---

## Overview
What, when, and why...

## [Domain-Specific Sections]
2-5 sections with guidelines, examples, standards...

## When to Apply
Trigger conditions...
```

### Progressive Disclosure

Skills use a three-tier loading system:
1. **Metadata** (YAML frontmatter) - Always loaded for relevance check
2. **Markdown body** - Loaded when skill is activated
3. **Resources** (REFERENCE.md, etc.) - Loaded only when needed

## Two-Tier Learning Systems (v5.9.0+)

### Agent Feedback System

**Purpose**: Enable explicit feedback exchange between analysis and execution agents for continuous improvement.

**Location**: `lib/agent_feedback_system.py` + `.claude-patterns/agent_feedback.json`

**How It Works**:
1. **Tier 1 Agent** (e.g., code-analyzer) provides recommendations
2. **Tier 2 Agent** (e.g., quality-controller) executes and evaluates
3. **Feedback Captured**: Effectiveness, quality improvement, recommendations followed
4. **Learning Applied**: Tier 1 learns what recommendations work best

**Key Features**:
- Feedback types: success, improvement, warning, error
- Collaboration matrix tracking
- Effectiveness metrics
- Learning insights extraction

**Example**:
```python
# Tier 2 provides feedback to Tier 1
add_feedback(
  from_agent="quality-controller",
  to_agent="code-analyzer",
  task_id="pattern_20251104_001",
  feedback_type="success",
  message="Recommendations were highly effective. Quality score improved +12 points",
  impact="quality_score +12"
)
```

### Agent Performance Tracking

**Purpose**: Track individual agent performance metrics for specialization identification and continuous improvement.

**Location**: `lib/agent_performance_tracker.py` + `.claude-patterns/agent_performance.json`

**Metrics Tracked**:
- Success rate per agent
- Average quality score
- Average execution time
- Task specializations
- Performance trends (improving/declining/stable)
- Recommendations followed (for Tier 1 agents)
- Auto-fix success rate (for Tier 2 agents)

**Key Features**:
- Top performer identification
- Weak performer detection
- Specialization discovery (agent X excels at refactoring)
- Performance rating (Excellent/Good/Satisfactory/Needs Improvement/Poor)

**Example**:
```python
record_task_execution(
  agent_name="test-engineer",
  task_id="task_123",
  task_type="testing",
  success=True,
  quality_score=94.0,
  execution_time_seconds=120,
  iterations=1
)
# Result: test-engineer identified as "Excellent" performer for testing tasks
```

### User Preference Learning

**Purpose**: Learn user preferences over time to adapt agent behavior for personalized experience.

**Location**: `lib/user_preference_learner.py` + `.claude-patterns/user_preferences.json`

**Preferences Learned**:

**Coding Style**:
- Verbosity (concise/balanced/verbose)
- Comment level (minimal/moderate/extensive)
- Documentation level (minimal/standard/comprehensive)

**Workflow Preferences**:
- Auto-fix confidence threshold (0.85-0.95)
- Confirmation required for: breaking_changes, security_fixes
- Parallel execution preference
- Quality threshold

**Quality Weights**:
- Tests importance (0-1)
- Documentation importance (0-1)
- Code quality importance (0-1)
- Standards importance (0-1)
- Patterns importance (0-1)

**Communication Style**:
- Detail level (brief/balanced/detailed)
- Technical depth (low/medium/high)
- Explanation preference (minimal/when_needed/always)

**How It Learns**:
```python
# User approves changes
record_interaction(
  interaction_type="approval",
  task_id="task_456",
  user_feedback="Good",
  context={
    "code_style": {"verbosity": "concise"},
    "quality_focus": {"tests": 0.40, "documentation": 0.25}
  }
)
# Result: System adapts to prefer concise code and higher test coverage
```

**Learning Confidence**: Increases with more interactions (0-100%)

### Integration with Dashboard

All three systems integrate seamlessly with the monitoring dashboard:
- **Agent Feedback**: Shows collaboration effectiveness
- **Agent Performance**: Displays top/weak performers, trends
- **User Preferences**: Shows learned patterns and confidence level

## Autonomous Workflow Example (Two-Tier)

```
User: "Refactor the authentication module"
    â†“
Orchestrator (two-tier autonomous execution):

    === TIER 1: ANALYSIS ===
    1. Loads user preferences â†’ Prefers concise code, tests priority 40%
    2. Delegates to code-analyzer â†’ Analyzes auth module structure
    3. Delegates to security-auditor â†’ Identifies security concerns
    4. Delegates to smart-recommender â†’ Suggests optimal approach
    5. Collects recommendations â†’ 3 recommendations with confidence scores

    === TIER 2: EXECUTION ===
    6. Evaluates recommendations â†’ Filters by user preferences
    7. Delegates to quality-controller â†’ Executes refactoring with context
    8. Auto quality check â†’ Score: 96/100 âœ“
    9. Records agent performance â†’ quality-controller: +1 success

    === FEEDBACK LOOP ===
    10. Tier 2 â†’ Tier 1 feedback â†’ "Recommendations effective, +12 quality"
    11. Records user approval â†’ Updates preference: refactoring style
    12. Stores pattern â†’ For future similar tasks
    13. Updates agent specializations â†’ code-analyzer excels at auth

    === RESULT ===
    14. Returns â†’ Refactored code + quality report + learned preferences
```

## Quality Control Integration

**Auto-Fix Loop** (when quality < 70):
1. quality-controller runs comprehensive checks
2. Identifies failures (tests, standards, docs, patterns)
3. Auto-delegates fixes:
   - Failing tests â†’ test-engineer
   - Style violations â†’ Auto-format
   - Missing docs â†’ documentation-generator
4. Re-runs quality assessment
5. Iterates until quality â‰¥ 70 or max 3 attempts

## Pattern Learning Integration

**Every task execution automatically**:
1. Monitors skills loaded and agents delegated
2. Records execution approach and duration
3. Calculates quality score after completion
4. Stores pattern to `.claude-patterns/patterns.json`
5. Updates skill effectiveness metrics
6. Updates agent effectiveness metrics

**Future tasks benefit**:
- Query pattern database for similar tasks
- Reuse successful skill combinations
- Avoid approaches that failed previously
- Continuously improve recommendations

## File Organization Rules

- **Agents**: One file per agent in `agents/` directory (auto-discovered)
- **Skills**: One directory per skill in `skills/` with `SKILL.md` inside (auto-discovered)
- **Commands**: One file per command in `commands/` directory (auto-discovered)
- **Plugin manifest**: `.claude-plugin/plugin.json` contains metadata only
- **Documentation**: Keep README.md, STRUCTURE.md, docs/implementation/IMPLEMENTATION_SUMMARY.md up to date

## Testing and Validation

When modifying this plugin:

1. **Validate JSON syntax**: `plugin.json` must be valid JSON with recognized keys only
2. **Check YAML frontmatter**: All agents/skills must have valid YAML
3. **Verify file structure**: Ensure agents/, skills/, and commands/ directories are properly organized
4. **Test agent descriptions**: Descriptions should be action-oriented for auto-delegation
5. **Verify skill references**: Ensure agents reference existing skills correctly

## Common Development Tasks

This repository contains **configuration files** (Markdown + JSON), not executable code. There are no build, test, or lint commands. The plugin is "tested" by:
1. Installing in Claude Code
2. Using it in actual projects
3. Verifying autonomous behavior and pattern learning

## Integration Points

### Orchestrator â†” Specialized Agents
- Orchestrator delegates based on task analysis
- Agents execute with focused expertise using referenced skills
- Results flow back to orchestrator for quality assessment

### Agents â†” Skills
- Agents reference skills in system prompts by name
- Skills are loaded via progressive disclosure
- Skills provide domain knowledge without context isolation

### Pattern Database â†” All Components
- All agents contribute to pattern storage
- Orchestrator queries patterns for decision-making
- Continuous improvement feedback loop

## Background Tasks Architecture

The `background-task-manager` agent enables parallel execution:
- Security scanning while refactoring
- Documentation updates while coding
- Performance profiling while optimizing
- Non-blocking execution with result integration

## Automatic Learning System (v1.1+)

### Learning-Engine Agent

**Purpose**: Silently captures patterns and continuously improves performance after every task

**Triggers**: Automatically invoked by orchestrator after each task completion

**What It Does**:
1. Captures task context (type, language, framework, complexity)
2. Records execution details (skills used, agents delegated, approach taken)
3. Stores outcome metrics (quality score, success/failure, performance)
4. Updates skill effectiveness metrics in real-time
5. Updates agent performance metrics
6. Analyzes trends every 10 tasks
7. Optimizes configurations every 25 tasks

**Key Feature**: Completely silent - no user-facing output, pure background learning

**Integration**:
```javascript
// Orchestrator automatically does this after every task:
await complete_task(task_data)
await assess_quality(result)
await delegate_to_learning_engine(task_data)  // SILENT
return result_to_user
```

### Adaptive Skill Selection

Based on learning data, the orchestrator now:
- Queries pattern database for similar successful tasks
- Ranks skills by historical success rate for task type
- Auto-loads optimal skill combinations
- Avoids skills with poor performance for specific task types
- Continuously improves recommendations based on outcomes

### Performance Improvements

With learning enabled:
- Quality scores improve 15-20% after 10 similar tasks
- Execution time decreases ~20% through optimization
- Skill selection accuracy increases from 70% to 92%
- Auto-fix success rate improves from 65% to 85%

## Python Utility Libraries (v1.4+)

### Cross-Platform Compatibility

All Python scripts in `lib/` directory feature Windows compatibility (v1.4):
- **File Locking**: Automatic platform detection uses `msvcrt` on Windows, `fcntl` on Unix/Linux/Mac
- **Error Handling**: Enhanced exception catching for platform-specific issues
- **Path Handling**: Works with both forward slashes and backslashes

### Available Utilities

**pattern_storage.py**:
- Manages learned patterns in `.claude-patterns/patterns.json`
- Thread-safe read/write with file locking
- Pattern retrieval with relevance scoring
- Used by learning-engine agent

**task_queue.py**:
- Priority-based task management
- Status tracking (pending/running/completed/failed)
- Used for coordinating background tasks
- CLI interface for manual task management

**quality_tracker.py**:
- Records quality metrics over time
- Trend analysis (improving/stable/declining)
- Multi-metric tracking (code, tests, docs, patterns)
- Used by quality-controller agent

### Integration Notes

- All scripts use `--dir` parameter to specify data directory (default: `.claude-patterns`)
- JSON-based data storage for easy inspection and debugging
- Each script has a complete CLI interface accessible via `python <plugin_path>/lib/<script>.py --help`
- Scripts are designed to be used both programmatically and via command line

## Result Presentation Requirements

**CRITICAL**: Use **two-tier presentation strategy** for slash commands. Follow the guidelines in `docs/guidelines/RESULT_PRESENTATION_GUIDELINES.md`.

**Tier 1: Concise Terminal Output (15-20 lines max)**:
- Status line with key metric
- Top 3 findings only
- Top 3 recommendations only
- File path to detailed report
- Execution time

**Tier 2: Detailed File Report (comprehensive)**:
- Save to `.claude/reports/[command]-YYYY-MM-DD.md`
- Include ALL findings, metrics, charts, visualizations
- Complete recommendations and analysis
- Full formatting with boxes and sections

**Critical Rules**:
- Terminal = Quick summary only (15-20 lines max)
- File = Complete details with all findings
- Always include file path in terminal output
- Never complete silently, never show 50+ lines in terminal

**See**: `docs/guidelines/RESULT_PRESENTATION_GUIDELINES.md` for complete formatting standards and examples.

## Validation System (v1.7+)

### Purpose
Prevents tool usage errors, maintains documentation consistency, and ensures compliance with Claude Code best practices **before errors occur**.

### Key Features

**Pre-flight Validation** (Before Operations):
- Validates Edit tool prerequisite (file must be read first)
- Checks Write tool safety (warns if overwriting without reading)
- Verifies path validity (directories exist)
- Ensures parameter completeness

**Post-error Validation** (After Failures):
- Detects error patterns ("File has not been read yet")
- Identifies root cause automatically
- Applies auto-fix (e.g., Read file first)
- Retries operation
- Stores failure pattern to prevent recurrence

**Documentation Validation** (After Updates):
- Checks version synchronization across files
- Detects path inconsistencies
- Verifies component count accuracy
- Validates cross-references
- Auto-fixes or alerts user

### Automatic Triggers

The orchestrator automatically triggers validation:
1. **Before Edit/Write**: Pre-flight check
2. **After tool errors**: Post-error analysis and auto-fix
3. **After doc changes**: Consistency check
4. **Every 25 tasks**: Comprehensive audit

### Performance Metrics

- **87% error prevention rate** - Errors caught before they occur
- **100% auto-fix success** - Common errors fixed automatically
- **Zero documentation drift** - Consistency maintained
- **50% faster debugging** - No manual investigation needed

### Validation Score

Calculated across 5 dimensions (0-100):
- Tool Usage Compliance (30 points)
- Documentation Consistency (25 points)
- Best Practices Adherence (20 points)
- Error-Free Execution (15 points)
- Pattern Compliance (10 points)

Threshold: 70/100 minimum

## Full-Stack Validation System (v2.0+)

### Purpose
Comprehensive validation and auto-fix for full-stack applications. Validates backend, frontend, database, and infrastructure in parallel while automatically fixing 80-90% of common issues.

### Key Features

**Full-Stack Validation** (`/validate:fullstack`):
- **Backend**: Dependencies, type hints, tests, API schema, database migrations, SQLAlchemy 2.0 compatibility
- **Frontend**: TypeScript, builds, dependencies, bundle size, ESM/CommonJS conflicts
- **API Contracts**: Frontend â†” Backend endpoint matching, type synchronization, error handling
- **Database**: Schema integrity, test isolation, query efficiency, CASCADE fixes
- **Infrastructure**: Docker services, environment variables, volume validation

**24 Auto-Fix Patterns** (`patterns/autofix-patterns.json`):
- **Always Auto-Fixed (12 patterns)**: unused imports, SQLAlchemy text(), ESM conflicts, missing configs
- **Suggested Fixes (12 patterns)**: React Query v4â†’v5, type hints, error handling, bundle optimization
- **89% Average Success Rate**: Validated across multiple project types

### New Specialized Agents

**frontend-analyzer**:
- TypeScript validation with auto-fix (unused imports, missing types)
- React Query syntax migration (v4 â†’ v5)
- Build configuration validation (Vite, Webpack, Rollup)
- Bundle size analysis with optimization recommendations
- ESM/CommonJS conflict resolution

**api-contract-validator**:
- Backend API schema extraction (OpenAPI/Swagger)
- Frontend API call discovery and matching
- Auto-generate TypeScript types from backend schemas
- Auto-generate missing API client methods
- Cross-validate parameter types and HTTP methods

**build-validator**:
- Build tool detection and config validation
- Environment variable tracking and .env.example generation
- Module system conflict detection (ESM vs CommonJS)
- Bundle analysis with optimization suggestions
- Auto-generate missing config files

**test-engineer (enhanced)**:
- Database test isolation validation
- SQLAlchemy 2.0 compatibility auto-fix (text() wrapper)
- Database CASCADE auto-fix for teardown issues
- pytest fixture generation
- View/trigger dependency detection

### Auto-Fix Decision Matrix

**Priority Levels**:
- `auto`: Fix automatically without confirmation (success rate > 90%)
- `suggest`: Suggest fix and ask for confirmation (success rate 70-90%)
- `report`: Report issue, manual fix required (success rate < 70%)

**Example Auto-Fixes**:
```python
# SQLAlchemy text() wrapper (100% success)
execute("SELECT *")  â†’  execute(text("SELECT *"))

# Database CASCADE (100% success)
DROP TABLE users;  â†’  DROP TABLE users CASCADE;

# TypeScript unused imports (100% success)
import { unused } from 'lib';  â†’  (removed)

# ESM in .js file (95% success)
postcss.config.js  â†’  postcss.config.mjs
```

### Validation Workflow

1. **Project Detection** (5-10s): Identify all tech stack components
2. **Parallel Validation** (30-120s): Backend, frontend, database simultaneously
3. **Cross-Component Validation** (15-30s): API contracts, env vars, auth flow
4. **Auto-Fix Application** (10-30s): Apply high-confidence fixes
5. **Quality Assessment** (5-10s): Calculate score and generate report

### Performance Metrics

- **Time Savings**: 45-60 min manual â†’ 2-3 min automated (93-95% reduction)
- **Auto-Fix Success**: 80-90% of issues fixed automatically
- **Issue Detection**: 92% after 10 similar projects (learns over time)
- **Quality Score**: 87/100 average for production-ready projects

### Integration with Learning System

Full-stack validation integrates with pattern learning:
- Captures project structure patterns (FastAPI + React + PostgreSQL)
- Learns common issue frequencies (unused imports, SQL text() wrapper)
- Stores auto-fix success rates for continuous improvement
- Optimizes validation workflow based on project type

### Quality Score Calculation (v2.0)

```
Total Score (0-100):
â”œâ”€ Component Scores (60 points):
â”‚  â”œâ”€ Backend: 20 points
â”‚  â”œâ”€ Frontend: 20 points
â”‚  â””â”€ Integration: 20 points
â”œâ”€ Test Coverage (15 points): 70%+ = 15
â”œâ”€ Auto-Fix Success (15 points): All fixed = 15
â””â”€ Best Practices (10 points): Docs, types, standards

Thresholds:
âœ… 70-100: Production Ready
âš ï¸  50-69: Needs Improvement
âŒ 0-49: Critical Issues
```

## Notes for Future Claude Instances

- **Result presentation**: ALWAYS show formatted results after slash commands - see `docs/guidelines/RESULT_PRESENTATION_GUIDELINES.md`
- **Pattern database location**: Always `.claude-patterns/` in user projects, NEVER in this plugin repo
- **Auto-creation**: Orchestrator creates pattern directory automatically on first use
- **Automatic learning**: learning-engine runs SILENTLY after every task - never show "learning..." messages to user
- **Automatic validation**: validation-controller runs AUTOMATICALLY before Edit/Write and after errors - prevents failures proactively
- **Autonomy is key**: Never ask for confirmation at each step - make decisions independently
- **Quality threshold**: 70/100 is the minimum acceptable quality score
- **Validation threshold**: 70/100 is the minimum acceptable validation score
- **Skill references**: Use skill directory names (e.g., "code-analysis"), not file paths
- **Agent tools**: If tools not specified in frontmatter, agent inherits all tools from main thread
- **Learning improves over time**: First 3-5 similar tasks build baseline, performance improves significantly from task 5+
- **Validation prevents errors**: Pre-flight checks catch 87% of errors before they occur - always validate before Edit/Write
